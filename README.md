# Head-wise Shareable Attention for Large Language Models
该仓库简单整理了EMNLP24-Findings论文"Head-wise Shareable Attention for Large Language Models"探索过程中的一些代码，还需进一步完善。<br>
https://arxiv.org/abs/2402.11819

## 引用 Citation
```
@article{cao2024head,
  title={Head-wise Shareable Attention for Large Language Models},
  author={Cao, Zouying and Yang, Yifei and Zhao, Hai},
  journal={arXiv preprint arXiv:2402.11819},
  year={2024}
}
```

## 论文总结 Summary of this Paper
### Motivation: From Attention Map Similarity to Weight Matrix Similarity
![similarity](https://github.com/user-attachments/assets/9ee63343-8206-4fd0-9b6b-79a9d7318f56)

### DirectShare and PostShare
![pipeline](https://github.com/user-attachments/assets/d9f4a572-f09f-4922-8782-f5371e788fed)



